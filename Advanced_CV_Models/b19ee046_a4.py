# -*- coding: utf-8 -*-
"""B19EE046_CV_Assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H0PP4DxInzpNTbC1hke6TapUX-YrDOGT
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Pip Install"""

!pip install pytorch_lightning

"""# Import"""

import os
import torch
from torch.utils.data import Dataset, random_split, DataLoader
import pytorch_lightning as pl
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from torchvision import models
import PIL
from tqdm import tqdm
import cv2
from sklearn.metrics import classification_report, roc_curve
import pandas as pd
import re
import string
from google.colab.patches import cv2_imshow
from sklearn.preprocessing import label_binarize

"""# Functions"""

def transform_fn(img):
  img = img.resize((128,128))
  tensor = transforms.ToTensor()(img)
  return tensor

def transform_fl(img):
  img = img.resize((64,64))
  tensor = transforms.ToTensor()(img)
  return tensor

def model_return(ckpt_path, model_obj):
  ckpt = torch.load(ckpt_path)
  model_obj.load_state_dict(ckpt['state_dict'])
  return model_obj.eval().cuda()

def confusion_matrix(pred,true):
  conf_matrix = np.zeros((10,10),dtype='uint8')
  for i in tqdm(range(len(pred))):
    conf_matrix[pred[i]][true[i]]+=1
  print(conf_matrix)

def accuracy(pred,true):
  class_vector = np.zeros((1,10))
  total_vector = np.zeros((1,10))
  total_correct = 0
  for i in range(len(pred)):
    if pred[i]==true[i]:
      total_correct+=1
      class_vector[0][true[i]]+=1
    total_vector[0][true[i]]+=1
  print("\nOverall Accuracy = ", 100*total_correct/len(pred), "%")
  print("\nClass-Wise Accuracies = ", 100*class_vector/total_vector)
  print(classification_report(pred,true))

def detect(test_loader, model_class):
  pred=[]
  true=[]
  proba=[]
  for i,j in tqdm(test_loader):
    i,j = i.cuda(), j.cuda()
    prob = model_class(i)
    out = prob.max(1, keepdim=True)[1]
    pred.append(out.detach().cpu().item())
    true.append(j.detach().cpu().item())
    proba.append(prob.detach().cpu().numpy())
  print("Confusion Matrix:\n")
  confusion_matrix(pred, true)
  accuracy(pred, true)
  del pred
  MultiClass_ROC(np.array(proba), true)
  del proba, true

def MultiClass_ROC(proba, true):
  fpr = {}
  tpr = {}
  thresh ={}
  n_class = 10
  plt.figure(figsize=(10,10))
  for i in range(n_class):
    fpr[i], tpr[i], thresh[i] = roc_curve(true, [proba[j][0,i] for j in range(proba.shape[0])], pos_label=i)
    plt.plot(fpr[i], tpr[i], label='Class '+str(i)+' vs Rest')
  plt.title('Multiclass ROC curve')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive rate')
  plt.legend(loc='best')
  plt.show()

"""#Question-1 """

class CNN_8Layer_Max(pl.LightningModule):
  
  def __init__(self, num_classes):
      super().__init__()
      self.model = nn.Sequential(
          nn.Conv2d(1,4,5),
          nn.MaxPool2d(2),
          nn.Conv2d(4,5,5),
          nn.MaxPool2d(2),
          nn.Conv2d(5,6,4),
          nn.MaxPool2d(2),
          nn.Flatten(),
          nn.Linear(1014,512),
          nn.Linear(512,num_classes)
      )
      self.loss = nn.CrossEntropyLoss()
      self.loss_accumulate=0
      self.loss_list=[]
  
  def forward(self, x):
      return self.model(x)
  
  def training_step(self, batch, batch_no):
      x, y = batch
      logits = self(x)
      loss = self.loss(logits, y)
      self.loss_accumulate+=loss
      if batch_no==599:
        self.loss_list.append(self.loss_accumulate)
        self.loss_accumulate=0
      return loss
  
  def loss_graph(self):
      plt.plot([x for x in range(0,20)],[y.detach().cpu() for y in self.loss_list])
      plt.show()
  
  def configure_optimizers(self):
      return torch.optim.Adam(self.parameters(),lr=0.001)

class CNN_8Layer_Power(pl.LightningModule):
  
  def __init__(self, num_classes):
      super().__init__()
      self.model = nn.Sequential(
          nn.Conv2d(1,4,5),
          nn.LPPool2d(2,2),
          nn.Conv2d(4,5,5),
          nn.LPPool2d(2,2),
          nn.Conv2d(5,6,4),
          nn.LPPool2d(2,2),
          nn.Flatten(),
          nn.Linear(1014,512),
          nn.Linear(512,num_classes)
      )
      self.loss = nn.CrossEntropyLoss()
      self.loss_accumulate=0
      self.loss_list=[]
  
  def forward(self, x):
      return self.model(x)
  
  def training_step(self, batch, batch_no):
      x, y = batch
      logits = self(x)
      loss = self.loss(logits, y)
      self.loss_accumulate+=loss
      if batch_no==599:
        self.loss_list.append(self.loss_accumulate)
        self.loss_accumulate=0
      return loss
  
  def loss_graph(self):
      plt.plot([x for x in range(0,20)],[y.detach().cpu() for y in self.loss_list])
      plt.show()
  
  def configure_optimizers(self):
      return torch.optim.Adam(self.parameters(),lr=0.001)

class CNN_8Layer_Avg(pl.LightningModule):
  
  def __init__(self, num_classes):
      super().__init__()
      self.model = nn.Sequential(
          nn.Conv2d(1,4,5),
          nn.AvgPool2d(2),
          nn.Conv2d(4,5,5),
          nn.AvgPool2d(2),
          nn.Conv2d(5,6,4),
          nn.AvgPool2d(2),
          nn.Flatten(),
          nn.Linear(1014,512),
          nn.Linear(512,num_classes)
      )
      self.loss = nn.CrossEntropyLoss()
      self.loss_accumulate=0
      self.loss_list=[]
  
  def forward(self, x):
      return self.model(x)
  
  def training_step(self, batch, batch_no):
      x, y = batch
      logits = self(x)
      loss = self.loss(logits, y)
      self.loss_accumulate+=loss
      if batch_no==599:
        self.loss_list.append(self.loss_accumulate)
        self.loss_accumulate=0
      return loss
  
  def loss_graph(self):
      plt.plot([x for x in range(0,20)],[y.detach().cpu() for y in self.loss_list])
      plt.show()
  
  def configure_optimizers(self):
      return torch.optim.Adam(self.parameters(),lr=0.001)

class Model(pl.LightningModule):

  def __init__(self, num_classes):
      super().__init__()
      self.model = models.resnet18(weights=None)
      self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      self.features = self.model.fc.in_features 
      self.model.fc = nn.Linear(self.features,num_classes)
      self.loss = nn.CrossEntropyLoss()
      self.loss_accumulate=0
      self.loss_list=[]
  
  def forward(self, x):
      return self.model(x)
  
  def training_step(self, batch, batch_no):
      x, y = batch
      logits = self(x)
      loss = self.loss(logits, y)
      self.loss_accumulate+=loss
      if batch_no==599:
        self.loss_list.append(self.loss_accumulate)
        self.loss_accumulate=0
      return loss
  
  def loss_graph(self):
      plt.plot([x for x in range(0,20)],[y.detach().cpu() for y in self.loss_list])
      plt.show()
  
  def configure_optimizers(self):
      return torch.optim.Adam(self.parameters(),lr=0.001)

train_MNIST = torch.utils.data.DataLoader(dataset=datasets.MNIST('../data', train=True, download=True, transform=transform_fn),batch_size=100, shuffle=True, pin_memory=True, num_workers=2)
test_MNIST = torch.utils.data.DataLoader(dataset=datasets.MNIST('../data', train=False, download=True, transform=transform_fn),batch_size=1, shuffle=True)

max_model = CNN_8Layer_Max(10).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q1/MaxPool', benchmark=True)
trainer.fit(max_model, train_MNIST)
max_model.loss_graph()
del trainer, max_model

Model_Test = model_return('/content/drive/MyDrive/CV_Assignment_4/Q1/MaxPool/lightning_logs/version_1/checkpoints/epoch=19-step=12000.ckpt', CNN_8Layer_Max(10))
detect(test_MNIST, Model_Test)
del Model_Test

LP_model = CNN_8Layer_Power(10).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q1/LPPool', benchmark=True)
trainer.fit(LP_model, train_MNIST)
LP_model.loss_graph()
del trainer, LP_model

Model_Test = model_return('/content/drive/MyDrive/CV_Assignment_4/Q1/LPPool/lightning_logs/version_0/checkpoints/epoch=19-step=12000.ckpt', CNN_8Layer_Power(10))
detect(test_MNIST, Model_Test)
del Model_Test

avg_model = CNN_8Layer_Avg(10).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q1/AvgPool', benchmark=True)
trainer.fit(avg_model, train_MNIST)
avg_model.loss_graph()
del trainer, avg_model

Model_Test = model_return('/content/drive/MyDrive/CV_Assignment_4/Q1/AvgPool/lightning_logs/version_0/checkpoints/epoch=19-step=12000.ckpt', CNN_8Layer_Avg(10))
detect(test_MNIST, Model_Test)
del Model_Test

resnet = Model(10).train().cuda()
trainer = pl.Trainer(accelerator='gpu', max_epochs=20, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q1/ResNet18', benchmark=True)
trainer.fit(resnet, train_MNIST)
resnet.loss_graph()
del trainer, resnet

Model_Test = model_return('/content/drive/MyDrive/CV_Assignment_4/Q1/ResNet18/lightning_logs/version_1/checkpoints/epoch=19-step=12000.ckpt', Model(10))
detect(test_MNIST, Model_Test)
del Model_Test

"""#Question-2"""

class CaptionVectorizer:
  def __init__(self, vocab, bos_token='<bos>', eos_token='<eos>', unk_token='<unk>'):
    self.vocab = vocab
    self.bos_token = bos_token
    self.eos_token = eos_token
    self.unk_token = unk_token
    self.bos_index = self.vocab.get(bos_token, len(self.vocab))
    if self.bos_index == len(self.vocab):
      self.vocab[bos_token] = self.bos_index
    self.eos_index = self.vocab.get(eos_token, len(self.vocab))
    if self.eos_index == len(self.vocab):
      self.vocab[eos_token] = self.eos_index
    self.unk_index = self.vocab.get(unk_token, len(self.vocab))
    if self.unk_index == len(self.vocab):
      self.vocab[unk_token] = self.unk_index

  def __call__(self, caption):
    caption = re.sub(r'http\S+', '', caption)
    caption = caption.lower().translate(str.maketrans('', '', string.punctuation))
    caption = ''.join([i for i in caption if not i.isdigit()])
    words = caption.split()
    indices = [self.bos_index]
    for word in words:
      if word in self.vocab:
        indices.append(self.vocab[word])
      else:
        indices.append(self.unk_index)
    indices.append(self.eos_index)
    return torch.LongTensor(indices)

  def return_vocab(self):
    return self.vocab

  def get_token_from_index(self, index):
    for token, idx in self.vocab.items():
      if idx == index:
        return token
    return self.unk_token

class creator(Dataset):
  def __init__(self, img_dir, dataframe, caption_vectorizer):
    self.img_dir = img_dir 
    self.dataframe = dataframe
    self.caption_vectorizer = caption_vectorizer

  def __getitem__(self, idx):
    self.img_name = self.dataframe.iloc[idx]['images']
    self.img_caption = self.dataframe.iloc[idx]['captions']
    self.img_path = os.path.join(self.img_dir,self.img_name)
    self.img = cv2.imread(self.img_path)
    if self.img is not None:
      self.img = cv2.resize(self.img,(224,224))
      self.vector = self.caption_vectorizer(self.img_caption)
      return transforms.ToTensor()(self.img), self.vector

  def __len__(self):
    return self.dataframe.shape[0]

class EncoderDecoder(pl.LightningModule):
  def __init__(self, cnn_name, mode, hidden_dim, output_dim, vocab_size):
    '''
    input_dim = number of channels (input to CNN)
    vocab_size = size of vocabulary (input to RNN's embedding layer)
    output_dim = number of classes (within vocabulary)
    '''
    super().__init__()
    self.loss=[]
    self.encoder_name = cnn_name
    self.mode = mode
    if self.encoder_name=='resnet_34' and self.mode=='Train':
      self.encoder = models.resnet34(weights='IMAGENET1K_V1')
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features
    if self.encoder_name=='convnext_tiny' and self.mode=='Train':
      self.encoder = models.convnext_tiny(weights='IMAGENET1K_V1')
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features 
    if self.encoder_name=='mnasnet0_5' and self.mode=='Train':
      self.encoder = models.mnasnet0_5(weights='IMAGENET1K_V1')
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features 
    if self.encoder_name=='regnet_y_400mf' and self.mode=='Train':
      self.encoder = models.regnet_y_400mf(weights='IMAGENET1K_V1')
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features 
    if self.encoder_name=='resnext50_32x4d' and self.mode=='Train':
      self.encoder = models.resnext50_32x4d(weights='IMAGENET1K_V1')
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features 
    if self.encoder_name=='shufflenet_v2_x0_5' and self.mode=='Train':
      self.encoder = models.shufflenet_v2_x0_5(weights='IMAGENET1K_V1')
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features 
    if self.encoder_name=='squeezenet1_0' and self.mode=='Train':
      self.encoder = models.squeezenet1_0(weights='IMAGENET1K_V1')
      self.features = self.encoder.classifier[-3].in_channels
      self.out = self.encoder.classifier[-3].out_channels
    if self.encoder_name=='vgg11' and self.mode=='Train':
      self.encoder = models.vgg11(weights='IMAGENET1K_V1')
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features 
    if self.encoder_name=='wide_resnet50_2' and self.mode=='Train':
      self.encoder = models.wide_resnet50_2(weights='IMAGENET1K_V1')
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features 
    if self.encoder_name=='swin_t' and self.mode=='Train':
      self.encoder = models.swin_t(weights='IMAGENET1K_V1')
      self.features = self.encoder.head.in_features
      self.out = self.encoder.head.out_features
    if self.encoder_name=='densenet_121' and self.mode=='Train':
      self.encoder = models.densenet121(weights='IMAGENET1K_V1')
      self.features = self.encoder.classifier.in_features
      self.out = self.encoder.classifier.out_features
    if self.encoder_name=='efficientnet_b0' and self.mode=='Train':
      self.encoder = models.efficientnet.efficientnet_b0(weights='IMAGENET1K_V1')
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features
    if self.encoder_name=='mobilenet_v3_large' and self.mode=='Train':
      self.encoder = models.mobilenet_v3_large(weights='IMAGENET1K_V1')
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features
    if self.encoder_name=='resnet_34' and self.mode=='Test':
      self.encoder = models.resnet34(weights=None)
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features
    if self.encoder_name=='convnext_tiny' and self.mode=='Test':
      self.encoder = models.convnext_tiny(weights=None)
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features 
    if self.encoder_name=='mnasnet0_5' and self.mode=='Test':
      self.encoder = models.mnasnet0_5(weights=None)
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features 
    if self.encoder_name=='regnet_y_400mf' and self.mode=='Test':
      self.encoder = models.regnet_y_400mf(weights=None)
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features 
    if self.encoder_name=='resnext50_32x4d' and self.mode=='Test':
      self.encoder = models.resnext50_32x4d(weights=None)
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features 
    if self.encoder_name=='shufflenet_v2_x0_5' and self.mode=='Test':
      self.encoder = models.shufflenet_v2_x0_5(weights=None)
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features 
    if self.encoder_name=='squeezenet1_0' and self.mode=='Test':
      self.encoder = models.squeezenet1_0(weights=None)
      self.features = self.encoder.classifier[-3].in_channels
      self.out = self.encoder.classifier[-3].out_channels
    if self.encoder_name=='vgg11' and self.mode=='Test':
      self.encoder = models.vgg11(weights=None)
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features 
    if self.encoder_name=='wide_resnet50_2' and self.mode=='Test':
      self.encoder = models.wide_resnet50_2(weights=None)
      self.features = self.encoder.fc.in_features
      self.out = self.encoder.fc.out_features 
    if self.encoder_name=='swin_t' and self.mode=='Test':
      self.encoder = models.swin_t(weights=None)
      self.features = self.encoder.head.in_features
      self.out = self.encoder.head.out_features
    if self.encoder_name=='densenet_121' and self.mode=='Test':
      self.encoder = models.densenet121(weights=None)
      self.features = self.encoder.classifier.in_features
      self.out = self.encoder.classifier.out_features
    if self.encoder_name=='efficientnet_b0' and self.mode=='Test':
      self.encoder = models.efficientnet.efficientnet_b0(weights=None)
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features
    if self.encoder_name=='mobilenet_v3_large' and self.mode=='Test':
      self.encoder = models.mobilenet_v3_large(weights=None)
      self.features = self.encoder.classifier[-1].in_features 
      self.out = self.encoder.classifier[-1].out_features
    for name, param in self.encoder.named_parameters():
        param.requires_grad = False
    self.embedding = nn.Embedding(vocab_size, self.out)
    self.decoder = nn.LSTM(2*self.out, hidden_dim, num_layers=1, batch_first=True)
    self.fc = nn.Linear(hidden_dim, output_dim)
    self.vocab_size = vocab_size
    self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)

  def forward(self, image, caption):
    caption = self.embedding(caption)
    image = self.encoder(image)
    image = image.unsqueeze(1).repeat(1, caption.size(1), 1)
    inputs = torch.cat((caption, image), dim=2)
    outputs, _ = self.decoder(inputs)
    outputs = self.fc(outputs)
    return outputs

  def training_step(self, batch, batch_idx):
    images, captions = batch
    outputs = self(images, captions[:,:-1])
    loss = self.loss_fn(outputs.reshape(outputs.shape[0]*outputs.shape[1], self.vocab_size-1), captions[:,1:].reshape(-1))
    if batch_idx==1874:
      print(loss)
    return loss

  def configure_optimizers(self):
    optimizer = optim.Adam(self.parameters(), lr=1e-3)
    return optimizer

def create(images_file=None, token_file=None):
  token_files = []
  token_captions = []
  if token_file is not None:
    with open(token_file, 'r') as f:
      for line in f:
        image_name, caption_text = line.strip().split('\t')
        image_name = image_name.split('#')[0]
        token_files.append(image_name)
        token_captions.append(caption_text)
  with open(images_file, 'r') as f:
    image_list = f.read().splitlines()
  return image_list, token_files, token_captions

def return_dataframe(reference_list, imgs, captions):
  token_imgs = []
  token_captions = []
  for item in reference_list:
    for img_index in range(len(imgs)):
      if imgs[img_index]==item:
        token_imgs.append(item)
        token_captions.append(captions[img_index])
  df = pd.DataFrame()
  df['images']=token_imgs
  df['captions']=token_captions
  return df

def collate_fn(data):
  images, captions = zip(*data)
  captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=0)
  return torch.stack(images), captions

def generate_caption(model, image_tensor, caption_vectorizer):
  model.eval().cuda()
  with torch.no_grad():
    features = model.encoder(image_tensor)
    features = features.unsqueeze(1)
    inputs = torch.LongTensor([[caption_vectorizer.bos_index]]).cuda()
    caption = []
    MAX_SEQ_LEN = 20
    for i in range(MAX_SEQ_LEN):
      caption.append(inputs.item())
      embeddings = model.embedding(inputs)
      inputs = torch.cat((features, embeddings), dim=2)
      outputs, hidden = model.decoder(inputs)
      predicted = outputs.argmax(-1)[:,-1]
      if predicted.item() == caption_vectorizer.eos_index:
        break
      inputs = predicted.unsqueeze(1)
  caption = [caption_vectorizer.get_token_from_index(w) for w in caption]
  return image_tensor.detach().cpu().numpy(), ' '.join(caption)

#cd /content/drive/MyDrive

#!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip
#!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip
#!unzip Flickr8k_Dataset.zip
#!unzip Flickr8k_text.zip

'''train_imgs, token_imgs , token_captions = create('/content/drive/MyDrive/CV_Assignment_4/Q2/Flickr_8k.trainImages.txt','/content/drive/MyDrive/CV_Assignment_4/Q2/Flickr8k.token.txt')
test_imgs, _, _ = create('/content/drive/MyDrive/CV_Assignment_4/Q2/Flickr_8k.testImages.txt',None)
df_train = return_dataframe(train_imgs, token_imgs, token_captions)
df_test = return_dataframe(test_imgs, token_imgs, token_captions)
df_train.to_csv('/content/drive/MyDrive/CV_Assignment_4/Q2/train_dataframe.csv')
df_test.to_csv('/content/drive/MyDrive/CV_Assignment_4/Q2/test_dataframe.csv')'''

df_train = pd.read_csv('/content/drive/MyDrive/CV_Assignment_4/Q2/train_dataframe.csv')

df_test = pd.read_csv('/content/drive/MyDrive/CV_Assignment_4/Q2/test_dataframe.csv')

df_train_list = df_train['captions'].to_list()
vocabulary={}
for caption in df_train_list:
  caption = re.sub(r'http\S+', '', caption)
  caption = caption.lower().translate(str.maketrans('', '', string.punctuation))
  caption = ''.join([i for i in caption if not i.isdigit()])
  words = caption.split()
  for word in words:
    if word not in vocabulary:
      vocabulary[word] = len(vocabulary)
caption_vectorizer = CaptionVectorizer(vocabulary)
print(len(caption_vectorizer.return_vocab().keys()))
print(caption_vectorizer.return_vocab())

#CV_Assignment_4/Q2/
train_dataset = creator('/content/drive/MyDrive/CV_Assignment_4/Q2/Flicker8k_Dataset',df_train, caption_vectorizer)
test_dataset = creator('/content/drive/MyDrive/CV_Assignment_4/Q2/Flicker8k_Dataset',df_test, caption_vectorizer)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=1, pin_memory=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)

"""## Training Models

### Squeezenet1_0
"""

model = EncoderDecoder(cnn_name='squeezenet1_0', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=10, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Models/squeezenet1_0/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

"""### Resnet_34"""

model = EncoderDecoder(cnn_name='resnet_34', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=10, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Models/resnet_34/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

model = EncoderDecoder(cnn_name='resnet_34', hidden_dim=512, mode='Train', output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=50, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q2/Models/resnet_34/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

"""### Swin_t"""

model = EncoderDecoder(cnn_name='swin_t', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=10, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q2/Models/swin_t/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

"""### MobileNet_v3_large"""

model = EncoderDecoder(cnn_name='mobilenet_v3_large', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=10, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q2/Models/mobilenet_v3_large/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

"""### Densenet_121"""

model = EncoderDecoder(cnn_name='densenet_121', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=10, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q2/Models/densenet_121/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

model = EncoderDecoder(cnn_name='densenet_121', hidden_dim=512, mode='Train', output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=50, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Models/densenet_121/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

"""### VGG_11"""

model = EncoderDecoder(cnn_name='vgg11', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=10, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q2/Models/vgg11/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

model = EncoderDecoder(cnn_name='vgg11', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=50, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q2/Models/vgg11/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

"""### EfficientNet_B0"""

model = EncoderDecoder(cnn_name='efficientnet_b0', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab))
trainer = pl.Trainer(accelerator='gpu', precision=16, max_epochs=10, default_root_dir='/content/drive/MyDrive/CV_Assignment_4/Q2/Models/efficientnet_b0/', benchmark=True)
trainer.fit(model, train_loader)
del model, trainer

"""## Seeing Captions on the Models"""

imgo = 0
for i,j in test_loader:
  imgo = i.cuda()
  break

model = model_return('/content/drive/MyDrive/CV_Assignment_4/Q2/Models/squeezenet1_0/lightning_logs/version_0/checkpoints/epoch=9-step=18750.ckpt',EncoderDecoder(cnn_name='squeezenet1_0', mode='Test', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab)))
img, caption = generate_caption(model, imgo, caption_vectorizer)
print(caption)
cv2_imshow(img[0].transpose((1,2,0))*255)

model = model_return('/content/drive/MyDrive/CV_Assignment_4/Q2/Models/densenet_121/lightning_logs/version_0/checkpoints/epoch=9-step=18750.ckpt',EncoderDecoder(cnn_name='densenet_121', mode='Test', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab)))
img, caption = generate_caption(model, imgo, caption_vectorizer)
print(caption)
cv2_imshow(img[0].transpose((1,2,0))*255)

model = model_return('/content/drive/MyDrive/CV_Assignment_4/Q2/Models/efficientnet_b0/lightning_logs/version_0/checkpoints/epoch=9-step=18750.ckpt',EncoderDecoder(cnn_name='efficientnet_b0', mode='Test', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab)))
img, caption = generate_caption(model, imgo, caption_vectorizer)
print(caption)
cv2_imshow(img[0].transpose((1,2,0))*255)

model = model_return('/content/drive/MyDrive/CV_Assignment_4/Q2/Models/mobilenet_v3_large/lightning_logs/version_1/checkpoints/epoch=9-step=18750.ckpt',EncoderDecoder(cnn_name='mobilenet_v3_large', mode='Test', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab)))
img, caption = generate_caption(model, imgo, caption_vectorizer)
print(caption)
cv2_imshow(img[0].transpose((1,2,0))*255)

model = model_return('/content/drive/MyDrive/CV_Assignment_4/Q2/Models/resnet_34/lightning_logs/version_1/checkpoints/epoch=49-step=93750.ckpt',EncoderDecoder(cnn_name='resnet_34', mode='Test', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab)))
img, caption = generate_caption(model, imgo, caption_vectorizer)
print(caption)
cv2_imshow(img[0].transpose((1,2,0))*255)

model = model_return('/content/drive/MyDrive/CV_Assignment_4/Q2/Models/swin_t/lightning_logs/version_0/checkpoints/epoch=9-step=18750.ckpt',EncoderDecoder(cnn_name='swin_t', mode='Test', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab)))
img, caption = generate_caption(model, imgo, caption_vectorizer)
print(caption)
cv2_imshow(img[0].transpose((1,2,0))*255)

model = model_return('/content/drive/MyDrive/CV_Assignment_4/Q2/Models/vgg11/lightning_logs/version_0/checkpoints/epoch=9-step=18750.ckpt',EncoderDecoder(cnn_name='vgg11', mode='Test', hidden_dim=512, output_dim=caption_vectorizer.unk_index, vocab_size=len(caption_vectorizer.vocab)))
img, caption = generate_caption(model, imgo, caption_vectorizer)
print(caption)
cv2_imshow(img[0].transpose((1,2,0))*255)

"""# Question-3"""

#!wget https://pjreddie.com/media/files/yolov3-tiny.weights

#!wget https://pjreddie.com/media/files/yolov3.weights

#!wget https://pjreddie.com/media/files/yolov2-tiny.weights

#!git clone https://github.com/AlexeyAB/darknet

#cd darknet

#!make

#!cp -r /content/darknet /content/drive/MyDrive/CV_Assignment_4/Q3/darknet_template

#!cp -r /content/drive/MyDrive/CV_Assignment_4/Q3/darknet_template /content/darknet

!chmod +x ./drive/MyDrive/CV_Assignment_4/Q3/darknet_template/darknet

cd /content/drive/MyDrive/CV_Assignment_4/Q3/darknet_template

"""## Yolov3-tiny"""

! ./darknet detector train data/deer.data cfg/yolov3-tiny_deer.cfg yolov3-tiny.weights -map -dont_show -mjpeg_port 8090 -map_visual -clear

"""### Threshold=0.05"""

! ./darknet detector test data/deer.data cfg/yolov3-tiny_deer.cfg backup/yolov3-tiny_deer_best.weights -dont_show -thresh 0.05 -ext_output -out > results_3tiny_005.txt

"""## Yolov3-320"""

! ./darknet detector train data/deer.data cfg/yolov3_deer.cfg yolov3.weights -map -dont_show -mjpeg_port 8090 -map_visual -clear

"""### Threshold=0.1"""

! ./darknet detector test data/deer.data cfg/yolov3_deer.cfg backup/yolov3_deer_last.weights -dont_show -thresh 0.1 -ext_output -out > results_3_01.txt

"""## Yolov2-tiny"""

! ./darknet detector train data/deer.data cfg/yolov2-tiny_deer.cfg yolov2-tiny.weights -map -dont_show -mjpeg_port 8090 -map_visual -clear

"""### Threshold=0.05"""

! ./darknet detector test data/deer.data cfg/yolov2-tiny_deer.cfg backup/yolov2-tiny_deer_best.weights -dont_show -thresh 0.05 -ext_output -out > results_2tiny_005.txt